{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Th6gTo_1vt1n"
   },
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mDreCUSLvt1o"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "numeric_cols = [\n",
    "    'Age',\n",
    "    'Quantity', 'Fee', 'State',\n",
    "    'VideoAmt', 'PhotoAmt', 'Type',\n",
    "    \"SentimentMagnitude\", \"SentimentScore\", \"NumSentences\"\n",
    "]\n",
    "\n",
    "one_hot_cols = {\n",
    "    #'Type': 2, \n",
    "    'Breed1': 307, 'Breed2': 307,\n",
    "    'Gender': 3, 'Color1': 7, 'Color2': 7,\n",
    "    'Color3': 7,\n",
    "    'MaturitySize': 5,\n",
    "    'FurLength': 4, 'Vaccinated': 3,\n",
    "    'Dewormed': 3, 'Sterilized': 3,\n",
    "    'Health': 4, 'State': 15\n",
    "}\n",
    "\n",
    "def one_hot_encode(df, col, num_class=None, labels=None, inplace=False):\n",
    "    ''' Takes in dataframe df and replaces col with num_class columns\n",
    "        For example, use as follows\n",
    "        for col, num_class in data.one_hot_cols.items():\n",
    "            one_hot_encode(train_df, col, num_class)\n",
    "    '''\n",
    "    # get the true values from data\n",
    "    column_values = np.sort(df[col].dropna().unique())\n",
    "    if num_class == None:\n",
    "        num_class = len(column_values)\n",
    "    if num_class == 2:\n",
    "        # These can just be boolean\n",
    "        if inplace:\n",
    "            df[col] = (df[col] == column_values[0]).astype(int)\n",
    "        else:\n",
    "            return (df[col] == column_values[0]).astype(int)\n",
    "    else:        \n",
    "        if labels is not None:\n",
    "            res = np.zeros((len(df), num_class))\n",
    "            for i, label in enumerate(labels):\n",
    "                if inplace:\n",
    "                    df[col+'_'+str(label)] = (df[col] == label).astype(int)\n",
    "                else:\n",
    "                    one_hot = np.zeros(num_class)\n",
    "                    one_hot[i] = 1\n",
    "                    res[df[col] == label] = one_hot\n",
    "        else:\n",
    "            res = np.zeros((len(df), num_class))\n",
    "            for i in range(num_class):\n",
    "                if (i >= len(column_values)):\n",
    "                    break # Index out of bounds\n",
    "                cur_value = column_values[i]\n",
    "\n",
    "                if inplace:\n",
    "                    df[col+'_'+str(cur_value)] = (df[col] == cur_value).astype(int)\n",
    "                else:\n",
    "                    one_hot = np.zeros(num_class)\n",
    "                    one_hot[i] = 1\n",
    "                    res[df[col] == cur_value] = one_hot\n",
    "    \n",
    "        if inplace:\n",
    "            # delete original column\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "        else:\n",
    "            return res\n",
    "\n",
    "def load_data(fname):\n",
    "    return pd.read_csv(fname)\n",
    "\n",
    "def get_sentiment(df, sentiment_location):\n",
    "    ''' Parses the text sentiment metadata and adds a few additional\n",
    "        metrics to the specified dataframe.\n",
    "    '''\n",
    "    sentiment_files = glob(sentiment_location + \"/*\")\n",
    "\n",
    "    # Add some additional metrics from the sentiment files\n",
    "    for s_file in tqdm(sentiment_files, desc='Loading sentiment data'):\n",
    "        pet_id = s_file.split('/')[-1].split('.')[0]\n",
    "        with open(s_file) as json_file:\n",
    "            data = json.load(json_file)\n",
    "\n",
    "            df.loc[df[\"PetID\"] == pet_id, \"SentimentMagnitude\"] = data['documentSentiment']['magnitude']\n",
    "            df.loc[df[\"PetID\"] == pet_id, \"SentimentScore\"] = data['documentSentiment']['score']\n",
    "            df.loc[df[\"PetID\"] == pet_id, \"NumSentences\"] = len(data['sentences'])\n",
    "\n",
    "def load_pet_files(regdir):\n",
    "    \"\"\" Extracts all of the files associated with each pet listed\n",
    "    by the 'PetID' tag.\n",
    "\n",
    "    regdir - The directory containing the files\n",
    "\n",
    "    returns a dictionary containing keypairs (k, v) such that v\n",
    "    matches the regex (k\\-.*) where k is the key (a valid PetID).\n",
    "    \"\"\"\n",
    "    if os.path.isfile(os.path.join(regdir + 'picked_pictures.npy')):\n",
    "        print(\"Images loaded from existing file\")\n",
    "        return np.load(os.path.join(regdir + 'picked_pictures.npy'))\n",
    "    pfiles = {}\n",
    "\n",
    "    # Extract the pet names\n",
    "    for f in tqdm(os.listdir(regdir), desc='Loading Pet Files'):\n",
    "        # Extract the name\n",
    "        n = f[:f.index('-')]\n",
    "        url = os.path.join(regdir, f)\n",
    "        img = load_image(url)\n",
    "        if n in pfiles:\n",
    "            # Add to the entry\n",
    "            pfiles[n].append(img)\n",
    "\n",
    "        else:\n",
    "            # Add a new entry\n",
    "            pfiles[n] = [img]\n",
    "\n",
    "    return pfiles\n",
    "\n",
    "def load_train_data(is_train = True):\n",
    "    # Get the annotations for each pet\n",
    "    dta = load_data('../input/train/train.csv' if is_train else '../input/test/test.csv')\n",
    "     \n",
    "    # Get the pet pictures\n",
    "    petpics = {}#load_pet_files('../input/train_images/' if is_train else '../input/test_images')\n",
    "\n",
    "    # Get the state ids\n",
    "    states = load_data('../input/state_labels.csv')\n",
    "    states = states['StateID'].tolist()\n",
    "\n",
    "    # Load parsed sentiment\n",
    "    get_sentiment(dta, '../input/{}_sentiment/'.format('train' if is_train else 'test'))\n",
    "    \n",
    "    # NaN will become zero\n",
    "    dta = dta.fillna(0)\n",
    "\n",
    "    X_num = []\n",
    "    X_pic = []\n",
    "\n",
    "    Y = []\n",
    "    \n",
    "    # Build a single object to store the X values\n",
    "    X = [X_num, X_pic]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for k in one_hot_cols:\n",
    "        one_hot_encode(dta, k, inplace=True)\n",
    "        print('One hot encoded', k)\n",
    "        \"\"\"\n",
    "    \n",
    "    for i, row in tqdm(dta.iterrows(), desc='Processing data rows'):\n",
    "        # Save the numeric values\n",
    "        vals = row[numeric_cols]\n",
    "        \n",
    "        state = [x == row['State'] for x in states]\n",
    "        #assert(sum(state) == 1)\n",
    "        \n",
    "        # Add all of the valid one-hot encodings\n",
    "        #state = [row[k] for k in dta if any(q in k for q in one_hot_cols)]\n",
    "            \n",
    "        x = list(vals) + state\n",
    "        \n",
    "            \n",
    "        # Join sentiment on PetID\n",
    "        \"\"\"\n",
    "        s = sentiment[sentiment['PetID'] == row['PetID']]\n",
    "        \n",
    "        for col in s:\n",
    "            if col not in row:\n",
    "                row[col] = s.iloc[0][col]\n",
    "                x.append(row[col])\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save the pictures\n",
    "        if row['PetID'] in petpics:\n",
    "            X_pic.append(petpics[row['PetID']])\n",
    "        else:\n",
    "            X_pic.append([])\n",
    "            \n",
    "        # Save the data pair\n",
    "        X_num.append(np.array(x))\n",
    "        \n",
    "        if is_train:\n",
    "            Y.append(row['AdoptionSpeed'])\n",
    "        else:\n",
    "            Y.append(row['PetID'])\n",
    "\n",
    "    # Laziness\n",
    "    if len(X) == 1:\n",
    "        X = np.array(X[0])\n",
    "    else:\n",
    "        X = list(map(np.array, X))\n",
    "\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    return (X, Y)\n",
    "\n",
    "def load_image(img_file, size=64):\n",
    "    # print('img file:', img_file)\n",
    "    img = Image.open(img_file)\n",
    "    img = img.resize((size, size), Image.ANTIALIAS)\n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "b6sUBRbzvt1r",
    "outputId": "92ac5f2d-098d-4191-9c5e-6d14b48703ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Loading sentiment data: 100%|██████████| 14442/14442 [03:30<00:00, 68.47it/s]\n",
      "Processing data rows: 14993it [00:18, 799.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Get the data\n",
    "X, Y = load_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "EWNsY2aes3i5",
    "outputId": "bdb2528e-f30d-44e0-b710-928ff166cfb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14993, 25), (14993, 0)]\n",
      "(14993,)\n",
      "[3.0000e+00 1.0000e+00 1.0000e+02 4.1326e+04 0.0000e+00 1.0000e+00\n",
      " 2.0000e+00 2.4000e+00 3.0000e-01 6.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 1.0000e+00\n",
      " 0.0000e+00]\n",
      "2\n",
      "nan: 0\n"
     ]
    }
   ],
   "source": [
    "print([x.shape for x in X])\n",
    "print(Y.shape)\n",
    "print(X[0][0])\n",
    "print(Y[0])\n",
    "\n",
    "X[0] = np.nan_to_num(X[0])\n",
    "print('nan:', np.count_nonzero(np.isnan(X[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bP5N-nfKvt1v"
   },
   "outputs": [],
   "source": [
    "def shuffle(X, Y):\n",
    "    idxs = list(range(len(Y)))\n",
    "    \n",
    "    if isinstance(X, list):\n",
    "        for i in range(len(X)):\n",
    "            X[i] = X[i][idxs]\n",
    "    else:\n",
    "        X = X[idxs]\n",
    "\n",
    "    Y = Y[idxs]\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def split(X, Y, s=0.2):\n",
    "    s = int(s * len(Y))\n",
    "    \n",
    "    if isinstance(X, list):\n",
    "        X_train = [x[:-s] for x in X]\n",
    "        X_test = [x[-s:] for x in X]\n",
    "    else:\n",
    "        X_train = X[:-s]\n",
    "        X_test = X[-s:]\n",
    "\n",
    "    Y_train = Y[:-s]\n",
    "    Y_test = Y[-s:]\n",
    "\n",
    "    return (X_train, Y_train), (X_test, Y_test)\n",
    "\n",
    "\n",
    "def convert_for_all(X, Y):\n",
    "    # For single image training, make a datapoint for each image or the default zero image\n",
    "    Xs = [[], []]\n",
    "    Ys = []\n",
    "    for i in range(len(X[1])):\n",
    "        Xs[0].append(X[0][i])\n",
    "        if len(X[1][i]) == 0:\n",
    "            Xs[1].append(np.zeros((64, 64, 3)))\n",
    "            Ys.append(Y[i])\n",
    "        else:\n",
    "            # Make a datapoint for all images. We assume equal relevance of images\n",
    "            for img in X[1][i]:\n",
    "                Xs[0].append(X[0][i])\n",
    "                Xs[1].append(img)\n",
    "                Ys.append(Y[i])\n",
    "\n",
    "    X = list(map(np.array, Xs))\n",
    "    Y = np.array(Ys)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def convert_for_single_axis(X, Y, ax=0):\n",
    "    return X[ax], Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RkUUG-buDZ5z"
   },
   "outputs": [],
   "source": [
    "X, Y = shuffle(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xsCyyd2Fvt1x"
   },
   "source": [
    "### Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RKtOpAxGvt1y"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "class KaggleModel:\n",
    "    def __init__(self, model, train, test):\n",
    "        model.summary()\n",
    "        self.model = model\n",
    "        self.train_data = train\n",
    "        self.test_data = test\n",
    "\n",
    "    def compile(self):\n",
    "        \"\"\" Compiles the model. Should be defined by the user.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def train(self, epochs=1):\n",
    "        \"\"\" Default training behavior. Simply does a Model.fit(X, Y).\n",
    "        \"\"\"\n",
    "        # Get the training data\n",
    "        X_train, Y_train = self.train_data\n",
    "\n",
    "        checkpoint = ModelCheckpoint('model.h5')\n",
    "        \n",
    "        # Fit to the data\n",
    "        self.model.fit(X_train, Y_train, epochs=epochs, validation_data=self.test_data, callbacks=[checkpoint])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "\n",
    "def ResidualBlock(mdl):\n",
    "    x = Input(shape=mdl.input_shape[1:])\n",
    "\n",
    "    y = mdl(x)\n",
    "    y = Add()([x, y])\n",
    "\n",
    "    return Model(x, y)\n",
    "\n",
    "class ImageFreeModel(KaggleModel):\n",
    "    def __init__(self, train, test):\n",
    "        kernel = Sequential(name='image_free_encoder')\n",
    "        \n",
    "        kernel.add(BatchNormalization(input_shape=(25,)))\n",
    "        \n",
    "        kernel.add(Dense(128))\n",
    "        kernel.add(Activation('relu'))\n",
    "\n",
    "        model = Sequential(name='image_free')\n",
    "        model.add(kernel)\n",
    "        \n",
    "        model.add(Dense(64, activation='relu'))\n",
    "\n",
    "        # Labels are one of [0, 1, 2, 3, 4]\n",
    "        model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "        # Build using the built model\n",
    "        super().__init__(model, train, test)\n",
    "\n",
    "    def compile(self):\n",
    "        self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XcePWPI9vt11"
   },
   "source": [
    "### Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9TGnFj8Wvt12"
   },
   "outputs": [],
   "source": [
    "def train_model(mdl, X, Y, epochs=32):\n",
    "    # Shuffle the data\n",
    "    shuffle(X, Y)\n",
    "\n",
    "    # One hot encode the output\n",
    "    Y = to_categorical(Y)\n",
    "\n",
    "    # Validation split\n",
    "    (X_train, Y_train), (X_valid, Y_valid) = split(X, Y)\n",
    "\n",
    "    print('Training points:', len(Y_train))\n",
    "    print('Validation points:', len(Y_valid))\n",
    "    print('Total points:', len(Y))\n",
    "\n",
    "    clf = mdl((X_train, Y_train), (X_valid, Y_valid))\n",
    "\n",
    "    # Build the model\n",
    "    clf.compile()\n",
    "\n",
    "    # Fit to the data\n",
    "    clf.train(epochs=epochs)\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9L8-hyo8vt14"
   },
   "source": [
    "### Model Training\n",
    "\n",
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1516
    },
    "colab_type": "code",
    "id": "AAlJDEqfvt18",
    "outputId": "dbd0e901-03be-4138-8e8c-65c60de56aa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training points: 11995\n",
      "Validation points: 2998\n",
      "Total points: 14993\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_free_encoder (Sequenti (None, 128)               3428      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 12,009\n",
      "Trainable params: 11,959\n",
      "Non-trainable params: 50\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 11995 samples, validate on 2998 samples\n",
      "Epoch 1/16\n",
      "11995/11995 [==============================] - 1s 107us/step - loss: 1.4502 - acc: 0.3259 - val_loss: 1.4274 - val_acc: 0.3419\n",
      "Epoch 2/16\n",
      "11995/11995 [==============================] - 1s 69us/step - loss: 1.4191 - acc: 0.3462 - val_loss: 1.4193 - val_acc: 0.3469\n",
      "Epoch 3/16\n",
      "11995/11995 [==============================] - 1s 64us/step - loss: 1.4085 - acc: 0.3556 - val_loss: 1.4124 - val_acc: 0.3582\n",
      "Epoch 4/16\n",
      "11995/11995 [==============================] - 1s 53us/step - loss: 1.4019 - acc: 0.3631 - val_loss: 1.4174 - val_acc: 0.3512\n",
      "Epoch 5/16\n",
      "11995/11995 [==============================] - 1s 53us/step - loss: 1.3964 - acc: 0.3689 - val_loss: 1.4161 - val_acc: 0.3592\n",
      "Epoch 6/16\n",
      "11995/11995 [==============================] - 1s 54us/step - loss: 1.3938 - acc: 0.3756 - val_loss: 1.4072 - val_acc: 0.3642\n",
      "Epoch 7/16\n",
      "11995/11995 [==============================] - 1s 54us/step - loss: 1.3925 - acc: 0.3745 - val_loss: 1.4086 - val_acc: 0.3592\n",
      "Epoch 8/16\n",
      "11995/11995 [==============================] - 1s 61us/step - loss: 1.3894 - acc: 0.3715 - val_loss: 1.4090 - val_acc: 0.3652\n",
      "Epoch 9/16\n",
      "11995/11995 [==============================] - 1s 63us/step - loss: 1.3875 - acc: 0.3742 - val_loss: 1.4087 - val_acc: 0.3582\n",
      "Epoch 10/16\n",
      "11995/11995 [==============================] - 1s 63us/step - loss: 1.3866 - acc: 0.3756 - val_loss: 1.4048 - val_acc: 0.3582\n",
      "Epoch 11/16\n",
      "11995/11995 [==============================] - 1s 61us/step - loss: 1.3839 - acc: 0.3758 - val_loss: 1.4061 - val_acc: 0.3566\n",
      "Epoch 12/16\n",
      "11995/11995 [==============================] - 1s 62us/step - loss: 1.3840 - acc: 0.3752 - val_loss: 1.4079 - val_acc: 0.3586\n",
      "Epoch 13/16\n",
      "11995/11995 [==============================] - 1s 61us/step - loss: 1.3785 - acc: 0.3812 - val_loss: 1.4091 - val_acc: 0.3542\n",
      "Epoch 14/16\n",
      "11995/11995 [==============================] - 1s 63us/step - loss: 1.3772 - acc: 0.3831 - val_loss: 1.4050 - val_acc: 0.3676\n",
      "Epoch 15/16\n",
      "11995/11995 [==============================] - 1s 62us/step - loss: 1.3727 - acc: 0.3826 - val_loss: 1.4153 - val_acc: 0.3686\n",
      "Epoch 16/16\n",
      "11995/11995 [==============================] - 1s 63us/step - loss: 1.3738 - acc: 0.3829 - val_loss: 1.4122 - val_acc: 0.3632\n"
     ]
    }
   ],
   "source": [
    "# Attribute model data\n",
    "X_attr, Y_attr = convert_for_single_axis(X, Y, ax=0)\n",
    "\n",
    "# Image-free model\n",
    "attr_clf = ImageFreeModel\n",
    "\n",
    "# Train the model\n",
    "attr_clf = train_model(attr_clf, X_attr, Y_attr, epochs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A7P2rPR5UbhO"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6Cat105rUYtP",
    "outputId": "54df7d82-1b77-41f0-a868-36784a8e538b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sentiment data: 100%|██████████| 3865/3865 [00:45<00:00, 84.59it/s]\n",
      "Processing data rows: 3972it [00:04, 803.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get the test data\n",
    "X, ids = load_train_data(is_train=False)\n",
    "X = X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hWmhXvpJVGrW"
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "Y = attr_clf.model.predict(X)\n",
    "Y = np.argmax(Y, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1105
    },
    "colab_type": "code",
    "id": "o18H5BcoV4lG",
    "outputId": "96448e7a-e1c5-48dd-8f4b-b11705886529"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          PetID  AdoptionSpeed\n",
      "0     e2dfc2935              4\n",
      "1     f153b465f              4\n",
      "2     3c90f3f54              2\n",
      "3     e02abc8a3              4\n",
      "4     09f0df7d1              4\n",
      "5     0487529d4              2\n",
      "6     bae7c4b1c              4\n",
      "7     548bcf206              2\n",
      "8     0f82cea1e              4\n",
      "9     a3787f15e              4\n",
      "10    0113cedff              4\n",
      "11    0070b950a              4\n",
      "12    cbe2df167              4\n",
      "13    37a0b72a4              4\n",
      "14    669695dd6              4\n",
      "15    be85036be              2\n",
      "16    c2bbbdde2              4\n",
      "17    6a968e033              4\n",
      "18    3107d0aa7              4\n",
      "19    9e11f1974              4\n",
      "20    14bc519cd              4\n",
      "21    cee2cdc6b              4\n",
      "22    12799a2af              1\n",
      "23    34af29aab              4\n",
      "24    5bd1a9042              4\n",
      "25    04d1a03dc              4\n",
      "26    cdac529a3              4\n",
      "27    6b593bbe2              4\n",
      "28    9992b9fce              3\n",
      "29    8c03c9a3b              4\n",
      "...         ...            ...\n",
      "3942  9c8a01226              4\n",
      "3943  66fd6705a              4\n",
      "3944  e4a715be4              3\n",
      "3945  759a48292              4\n",
      "3946  57a0b3def              4\n",
      "3947  7c9f999a0              4\n",
      "3948  6387fd0b7              2\n",
      "3949  47ebd0e43              4\n",
      "3950  d2553fbf5              4\n",
      "3951  afe8eca60              4\n",
      "3952  8865451b9              4\n",
      "3953  31822feba              2\n",
      "3954  4beab1c3c              4\n",
      "3955  ea6d84c73              4\n",
      "3956  093f709d7              2\n",
      "3957  15dff57fc              1\n",
      "3958  a691ef2a1              2\n",
      "3959  eea8e2b57              2\n",
      "3960  e7197c3e4              1\n",
      "3961  34a2402b3              4\n",
      "3962  bb6aead09              1\n",
      "3963  8ec8828d8              4\n",
      "3964  2baa835f4              2\n",
      "3965  275ad28b9              4\n",
      "3966  2fec25579              2\n",
      "3967  ae57f8d52              4\n",
      "3968  83432904d              4\n",
      "3969  399013029              4\n",
      "3970  fd80b8c80              4\n",
      "3971  493ed84ae              4\n",
      "\n",
      "[3972 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Save the results to a file\n",
    "df = pd.DataFrame({\n",
    "    'PetID': ids,\n",
    "    'AdoptionSpeed':Y\n",
    "})\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "OurModels.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
